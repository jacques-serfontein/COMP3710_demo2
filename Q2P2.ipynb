{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"WARNING: CUDA not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's preprocess the data and apply some transforms so that our model has an easier time\n",
    "with training and testing.\n",
    "\n",
    "Training set:\n",
    "- ToTensor:\n",
    "    - converts to PyTorch tensor\n",
    "- Normalize(mean, std):\n",
    "    - Normalises each channel of the tensor image. Using the standard mean and std for CIFAR10 dataset.\n",
    "- RandomHorizontalFlip:\n",
    "    - Added as a form of data augmentation to make the model more robust to different spatial orientations.\n",
    "- RandomCrop(size, padding, padding_mode):\n",
    "    - Randomly crops image to size (size, size) and adds a padding of 4 pixels to all sides of the image.\n",
    "    - padding_mode=reflect just means that the padding uses reflection of the input array to fill the new pixels. e.g. {a, b, c} => {a, b, c, b, a} for padding=2. Helps model generalise better.\n",
    "\n",
    "Testing set:\n",
    "- Only need to convert to tensor and normalise, as augmentation outside of this shouldn't be done on testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise data\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4, padding_mode='reflect')\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training set has 391 instances\n",
      "Testing set has 100 instances\n"
     ]
    }
   ],
   "source": [
    "# Batch size\n",
    "batch_size_train = 128\n",
    "batch_size_test = 100\n",
    "\n",
    "# Create train and test data\n",
    "train_data = CIFAR10(root='C:/Users/Jacqu/Downloads/data/cifar10', train=True, download=True, transform=transform_train)\n",
    "test_data = CIFAR10(root='C:/Users/Jacqu/Downloads/data/cifar10', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size_train, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size_test, shuffle=False, num_workers=2)\n",
    "\n",
    "# Report split sizes\n",
    "print(f'Training set has {len(train_loader)} instances')\n",
    "print(f'Testing set has {len(test_loader)} instances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the ResNet architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "For complex tasks like image classification, one needs a deep CNN to get a model that performs well. The issue with this is that, as you add more layers to the NN, it becomes difficult to train and the accuracy starts to saturate and degrade. This is called the Vanishing Gradient problem, where the gradients that are used to update the network become extremely small (vanish) as they are backpropogated from the output layers to the earlier layers.\n",
    "\n",
    "Enter Residual Networks (ResNet)!\n",
    "\n",
    "![Alt text](image-5.png)\n",
    "\n",
    "ResNet solves the Vanishing Gradient problem by utilising skip connections, which allows alternate shortcut pats for the gradient to flow through. Another benefit of these connections is that they allow the model to learn the identity functions which ensure that a higher layer will perform at least as good as a lower layer (and not worse).\n",
    "\n",
    "### Residual Block\n",
    "A residual block (as shown below) is a stack of layers set in such a way that the output of a layer is taken and added to another layer deeper in the block. The non-linearity is then applied after adding it together with the corresponding layer in the main path.\n",
    "\n",
    "![Alt text](image-4.png)\n",
    "\n",
    "So, a residual network is just a stack of residual blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1 # Output will not be expanded - will have same number of planes/channels as input\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        # in_planes: number of input planes/channels\n",
    "        # planes: number of output planes/channels\n",
    "        # stride: stride of the convolutional layer\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # 1st Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        # Perform Batch Normalisation for stabilising training and improving generalisation\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # 2nd Convolutional Layer\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        # 2nd Batch Normalisation\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # Shortcut connection to other residual blocks.\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        # If the input shape is different from the output shape\n",
    "        # a 1x1 convolution followed by batch normalisation is added to the shortcut\n",
    "        # to match the dimensions.\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer of conv., batch norm. and relu activation\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Second layer of conv. and batch norm.\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        # Add shortcut to output (residual connection)\n",
    "        out += self.shortcut(x)\n",
    "\n",
    "        # Apply relu activation\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # Batch normalisation after initial convolutional layer\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Residual layers consisting of multiple residual blocks\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # For every one of two blocks in a layer, first will downsample by a factor of 2,\n",
    "        # the second one will compute the convolutional layer\n",
    "        strides = [stride] + [1]*(num_blocks-1) # First block may downsample the input, while remaining blocks keep resolution.\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolutional layer, batch normalisation and relu activation\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Create residual layers\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        # Average pooling layer\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "\n",
    "        # Flatten output\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our hyper-parameters and initialise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 40\n",
    "learning_rate = 0.1\n",
    "num_classes = 10\n",
    "\n",
    "# Initialise model\n",
    "model = ResNet18().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model No. of Parameters: 11173962\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model info\n",
    "print(\"Model No. of Parameters:\", sum([param.nelement() for param in model.parameters()]))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise loss and optimiser\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Piecewise learning rate scheduler\n",
    "total_step = len(train_loader)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=learning_rate, steps_per_epoch=total_step, epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking down the parameters:\n",
    "- SGD:\n",
    "    - lr = learning_rate: Controls how big the updates to the model parameters are.\n",
    "    - momentum = 0.9: Momentum is used to help the optimiser navigate through the parameter space\n",
    "    and avoid getting stuck in local minima. Accelerates SGD by adding a fraction of the update vector\n",
    "    of the past time step to the current update vector. In this case, 90% of the previous update\n",
    "    is included.\n",
    "    - weight_decay = 0.0005: This is used for regularisation, to prevent the model from fitting \n",
    "    noise in the training data. It adds a penalty term to the objective function, essentially shrinking\n",
    "    the weights during optimisation. Smaller weights are considered simpler, thus favoured to make\n",
    "    the model generalise better.\n",
    "- OneCycleLR:\n",
    "    - This is a learning rate schedule that aims to improve model performance by oscillating the\n",
    "    learning rate in one complete cycle within a range of lower and upper bounds. It is known to\n",
    "    help models reach better performance faster when compared to constant or step-decayed learning rates.\n",
    "    - steps_per_epoch = total_step: The number of steps (mini_batches) in each epoch. LR will be updated\n",
    "    once every mini-batch, so total LR updates would be total_step.\n",
    "    - epochs = num_epochs: Total number of epochs for training, where an epoch is one complete\n",
    "    forward pass and backward pass of all the training examples.\n",
    "\n",
    "Note: when removing the learning rate scheduler during experimentation, the training loss oscillated\n",
    "at around 0.3-0.4 and got 85% accuracy on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Training\n",
      "Epoch [1/40], Step [390/391], Loss: 1.0752\n",
      "Epoch [2/40], Step [390/391], Loss: 0.7851\n",
      "Epoch [3/40], Step [390/391], Loss: 0.7326\n",
      "Epoch [4/40], Step [390/391], Loss: 0.6564\n",
      "Epoch [5/40], Step [390/391], Loss: 0.3847\n",
      "Epoch [6/40], Step [390/391], Loss: 0.5393\n",
      "Epoch [7/40], Step [390/391], Loss: 0.5597\n",
      "Epoch [8/40], Step [390/391], Loss: 0.5094\n",
      "Epoch [9/40], Step [390/391], Loss: 0.4769\n",
      "Epoch [10/40], Step [390/391], Loss: 0.4238\n",
      "Epoch [11/40], Step [390/391], Loss: 0.3816\n",
      "Epoch [12/40], Step [390/391], Loss: 0.3724\n",
      "Epoch [13/40], Step [390/391], Loss: 0.3263\n",
      "Epoch [14/40], Step [390/391], Loss: 0.4168\n",
      "Epoch [15/40], Step [390/391], Loss: 0.2537\n",
      "Epoch [16/40], Step [390/391], Loss: 0.2535\n",
      "Epoch [17/40], Step [390/391], Loss: 0.2722\n",
      "Epoch [18/40], Step [390/391], Loss: 0.3198\n",
      "Epoch [19/40], Step [390/391], Loss: 0.2945\n",
      "Epoch [20/40], Step [390/391], Loss: 0.2399\n",
      "Epoch [21/40], Step [390/391], Loss: 0.3328\n",
      "Epoch [22/40], Step [390/391], Loss: 0.3775\n",
      "Epoch [23/40], Step [390/391], Loss: 0.3783\n",
      "Epoch [24/40], Step [390/391], Loss: 0.2301\n",
      "Epoch [25/40], Step [390/391], Loss: 0.3931\n",
      "Epoch [26/40], Step [390/391], Loss: 0.3406\n",
      "Epoch [27/40], Step [390/391], Loss: 0.2965\n",
      "Epoch [28/40], Step [390/391], Loss: 0.2545\n",
      "Epoch [29/40], Step [390/391], Loss: 0.2806\n",
      "Epoch [30/40], Step [390/391], Loss: 0.2596\n",
      "Epoch [31/40], Step [390/391], Loss: 0.1745\n",
      "Epoch [32/40], Step [390/391], Loss: 0.1196\n",
      "Epoch [33/40], Step [390/391], Loss: 0.1405\n",
      "Epoch [34/40], Step [390/391], Loss: 0.0961\n",
      "Epoch [35/40], Step [390/391], Loss: 0.1398\n",
      "Epoch [36/40], Step [390/391], Loss: 0.0340\n",
      "Epoch [37/40], Step [390/391], Loss: 0.0175\n",
      "Epoch [38/40], Step [390/391], Loss: 0.0054\n",
      "Epoch [39/40], Step [390/391], Loss: 0.0157\n",
      "Epoch [40/40], Step [390/391], Loss: 0.0098\n",
      "Training time: 891.85 sec or 14.86 min\n"
     ]
    }
   ],
   "source": [
    "# Construct scaler for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "print(\"> Training\")\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Mixed precision training\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Scale loss. Calls backward on scaled loss to create scaled gradients\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # First unscales the gradients of the optimiser's assigned parameters. If these gradients do not contain infs or NaNs,\n",
    "        # optimizer.step() is then called, otherwise, optimizer.step() is skipped.\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Updates the scale for next iteration\n",
    "        scaler.update()\n",
    "\n",
    "        # Backwards and optimise\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Print loss at specific step\n",
    "        if (i+1) == 390:\n",
    "            print(\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\".format(\n",
    "                epoch+1, num_epochs, i+1, total_step, loss.item()\n",
    "            ))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Training time: {:.2f} sec or {:.2f} min\".format(elapsed, elapsed / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Testing\n",
      "Test Accuracy: 94.34 %\n",
      "Testing time: 6.77 sec or 0.11 min\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "print(\"> Testing\")\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    correct= 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Count number of correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(\"Test Accuracy: {} %\".format(100 * correct / total))\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Testing time: {:.2f} sec or {:.2f} min\".format(elapsed, elapsed / 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
