{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"WARNING: CUDA not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's preprocess the data and apply some transforms so that our model has an easier time\n",
    "with training and testing.\n",
    "\n",
    "Training set:\n",
    "- ToTensor:\n",
    "    - converts to PyTorch tensor\n",
    "- Normalize(mean, std):\n",
    "    - Normalises each channel of the tensor image. Using the standard mean and std for CIFAR10 dataset.\n",
    "- RandomHorizontalFlip:\n",
    "    - Added as a form of data augmentation to make the model more robust to different spatial orientations.\n",
    "- RandomCrop(size, padding, padding_mode):\n",
    "    - Randomly crops image to size (size, size) and adds a padding of 4 pixels to all sides of the image.\n",
    "    - padding_mode=reflect just means that the padding uses reflection of the input array to fill the new pixels. e.g. {a, b, c} => {a, b, c, b, a} for padding=2. Helps model generalise better.\n",
    "\n",
    "Testing set:\n",
    "- Only need to convert to tensor and normalise, as augmentation outside of this shouldn't be done on testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise data\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4, padding_mode='reflect')\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training set has 391 instances\n",
      "Testing set has 100 instances\n"
     ]
    }
   ],
   "source": [
    "# Batch size\n",
    "batch_size_train = 128\n",
    "batch_size_test = 100\n",
    "\n",
    "# Create train and test data\n",
    "train_data = CIFAR10(root='C:/Users/Jacqu/Downloads/data/cifar10', train=True, download=True, transform=transform_train)\n",
    "test_data = CIFAR10(root='C:/Users/Jacqu/Downloads/data/cifar10', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size_train, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size_test, shuffle=False, num_workers=2)\n",
    "\n",
    "# Report split sizes\n",
    "print(f'Training set has {len(train_loader)} instances')\n",
    "print(f'Testing set has {len(test_loader)} instances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the ResNet architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "For complex tasks like image classification, one needs a deep CNN to get a model that performs well. The issue with this is that, as you add more layers to the NN, it becomes difficult to train and the accuracy starts to saturate and degrade. This is called the Vanishing Gradient problem, where the gradients that are used to update the network become extremely small (vanish) as they are backpropogated from the output layers to the earlier layers.\n",
    "\n",
    "Enter Residual Networks (ResNet)!\n",
    "\n",
    "![Alt text](image-5.png)\n",
    "\n",
    "ResNet solves the Vanishing Gradient problem by utilising skip connections, which allows alternate shortcut pats for the gradient to flow through. Another benefit of these connections is that they allow the model to learn the identity functions which ensure that a higher layer will perform at least as good as a lower layer (and not worse).\n",
    "\n",
    "### Residual Block\n",
    "A residual block (as shown below) is a stack of layers set in such a way that the output of a layer is taken and added to another layer deeper in the block. The non-linearity is then applied after adding it together with the corresponding layer in the main path.\n",
    "\n",
    "![Alt text](image-4.png)\n",
    "\n",
    "So, a residual network is just a stack of residual blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # 1st Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        # Perform Batch Normalisation for stabilising training and improving generalisation\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # 2nd Convolutional Layer\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        # 2nd Batch Normalisation\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # Shortcut connection to downsample residual\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        # If the input shape is different from the output shape\n",
    "        # a 1x1 convolution followed by batch normalisation is added to the shortcut\n",
    "        # to match the dimensions.\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer of conv., batch norm. and relu activation\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Second layer of conv. and batch norm.\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        # Add shortcut to output (residual connection)\n",
    "        out += self.shortcut(x)\n",
    "\n",
    "        # Apply relu activation\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # Pytorch has downsampling\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # For every one of two blocks in a layer, first will downsample by a factor of 2,\n",
    "        # the second one will compute the convolutional layer\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock, [2, 2, 2, 2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(ResidualBlock, [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 40\n",
    "learning_rate = 0.1\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model No. of Parameters: 11173962\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model info\n",
    "print(\"Model No. of Parameters:\", sum([param.nelement() for param in model.parameters()]))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD does not change learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Piecewise learning rate scheduler\n",
    "total_step = len(train_loader)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=learning_rate, steps_per_epoch=total_step, epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Training\n",
      "Epoch [1/40], Step [390/391], Loss: 1.1184\n",
      "Epoch [2/40], Step [390/391], Loss: 0.8407\n",
      "Epoch [3/40], Step [390/391], Loss: 0.8414\n",
      "Epoch [4/40], Step [390/391], Loss: 0.7977\n",
      "Epoch [5/40], Step [390/391], Loss: 0.5678\n",
      "Epoch [6/40], Step [390/391], Loss: 0.4835\n",
      "Epoch [7/40], Step [390/391], Loss: 0.4660\n",
      "Epoch [8/40], Step [390/391], Loss: 0.4158\n",
      "Epoch [9/40], Step [390/391], Loss: 0.5166\n",
      "Epoch [10/40], Step [390/391], Loss: 0.4298\n",
      "Epoch [11/40], Step [390/391], Loss: 0.4324\n",
      "Epoch [12/40], Step [390/391], Loss: 0.3191\n",
      "Epoch [13/40], Step [390/391], Loss: 0.4344\n",
      "Epoch [14/40], Step [390/391], Loss: 0.2941\n",
      "Epoch [15/40], Step [390/391], Loss: 0.3774\n",
      "Epoch [16/40], Step [390/391], Loss: 0.4038\n",
      "Epoch [17/40], Step [390/391], Loss: 0.4323\n",
      "Epoch [18/40], Step [390/391], Loss: 0.2430\n",
      "Epoch [19/40], Step [390/391], Loss: 0.3853\n",
      "Epoch [20/40], Step [390/391], Loss: 0.2560\n",
      "Epoch [21/40], Step [390/391], Loss: 0.2880\n",
      "Epoch [22/40], Step [390/391], Loss: 0.3388\n",
      "Epoch [23/40], Step [390/391], Loss: 0.3672\n",
      "Epoch [24/40], Step [390/391], Loss: 0.1935\n",
      "Epoch [25/40], Step [390/391], Loss: 0.2361\n",
      "Epoch [26/40], Step [390/391], Loss: 0.1604\n",
      "Epoch [27/40], Step [390/391], Loss: 0.2680\n",
      "Epoch [28/40], Step [390/391], Loss: 0.1572\n",
      "Epoch [29/40], Step [390/391], Loss: 0.2122\n",
      "Epoch [30/40], Step [390/391], Loss: 0.1838\n",
      "Epoch [31/40], Step [390/391], Loss: 0.1658\n",
      "Epoch [32/40], Step [390/391], Loss: 0.1111\n",
      "Epoch [33/40], Step [390/391], Loss: 0.1559\n",
      "Epoch [34/40], Step [390/391], Loss: 0.0907\n",
      "Epoch [35/40], Step [390/391], Loss: 0.0490\n",
      "Epoch [36/40], Step [390/391], Loss: 0.0227\n",
      "Epoch [37/40], Step [390/391], Loss: 0.0251\n",
      "Epoch [38/40], Step [390/391], Loss: 0.0152\n",
      "Epoch [39/40], Step [390/391], Loss: 0.0328\n",
      "Epoch [40/40], Step [390/391], Loss: 0.0406\n",
      "Training time: 1157.73 sec or 19.30 min\n"
     ]
    }
   ],
   "source": [
    "# Construct scaler for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "print(\"> Training\")\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Scale loss. Calls backward on scaled loss to create scaled gradients\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # First unscales the gradients of the optimiser's assigned parameters. If these gradients do not contain infs or NaNs,\n",
    "        # optimizer.step() is then called, otherwise, optimizer.step() is skipped.\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Updates the scale for next iteration\n",
    "        scaler.update()\n",
    "\n",
    "        # Backwards and optimise\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (i+1) == 390:\n",
    "            print(\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\".format(\n",
    "                epoch+1, num_epochs, i+1, total_step, loss.item()\n",
    "            ))\n",
    "\n",
    "        scheduler.step()\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Training time: {:.2f} sec or {:.2f} min\".format(elapsed, elapsed / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Testing\n",
      "Test Accuracy: 94.54 %\n",
      "Testing time: 7.24 sec or 0.12 min\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "print(\"> Testing\")\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    correct= 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Count number of correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(\"Test Accuracy: {} %\".format(100 * correct / total))\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Testing time: {:.2f} sec or {:.2f} min\".format(elapsed, elapsed / 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
