{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://www.youtube.com/watch?v=9OHlgDjaE2I&ab_channel=AI-SPECIALS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import LFWPeople\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((height, height)),\n",
    "    transforms.RandomHorizontalFlip(),  # 0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5, 0.5, 0.5],   # 0-1 to [-1,1], formula (x-mean)/std\n",
    "                        [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.images\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# normalise\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_train = X_train[:, :, :, np.newaxis]\n",
    "X_test = X_test[:, :, :, np.newaxis]\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "\n",
    "x_train_to_tensor = torch.from_numpy(X_train).to(torch.float32).permute(0, 3, 1, 2)\n",
    "y_train_to_tensor = torch.from_numpy(y_train).to(torch.long) \n",
    "x_test_to_tensor = torch.from_numpy(X_test).to(torch.float32).permute(0, 3, 1, 2)\n",
    "y_test_to_tensor = torch.from_numpy(y_test).to(torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_to_tensor, y_train_to_tensor)\n",
    "test_dataset = TensorDataset(x_test_to_tensor, y_test_to_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "# Create train and test data\n",
    "train_data = LFWPeople(root='./data', split=\"train\", download=True, transform=transform)\n",
    "test_data = LFWPeople(root='./data', split=\"test\", download=True, transform=transform)\n",
    "\n",
    "# Create train and test dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_dir, batch_size, random_seed=42, valid_size=0.1, shuffle=True, test=False):\n",
    "\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.5, 0.5, 0.5],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((100, 100)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "\n",
    "    if test:\n",
    "        dataset = LFWPeople(root='./data', split=\"test\", download=True, transform=transform)\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    # load the dataset\n",
    "    train_dataset = LFWPeople(root='./data', split=\"train\", download=True, transform=transform)\n",
    "\n",
    "    valid_dataset = LFWPeople(root='./data', split=\"train\", download=True, transform=transform)\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "# LFWPeople dataset \n",
    "train_loader, valid_loader = data_loader(data_dir='./data', batch_size=64)\n",
    "\n",
    "test_loader = data_loader(data_dir='./data', batch_size=64, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data._get_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data._get_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count = 9525\n",
    "test_count = 3708\n",
    "print(train_count+test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 1, 50, 37])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "inputs, label = next(dataiter)\n",
    "print(f'Input shape: {inputs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class for CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=5749):\n",
    "        super(CNN, self).__init__()\n",
    "        # Specify layers in network\n",
    "\n",
    "        # Input shape= 128, 3, 150, 150 \n",
    "        # [0] = bs, \n",
    "        # [1] = num_channels, \n",
    "        # [2] = height,\n",
    "        # [3] = width\n",
    "\n",
    "        # Output size after convolution filter\n",
    "        # ((w - f + 2P)/s) + 1\n",
    "        # w = 150\n",
    "        # f = 3 = kernel_size = size of filter\n",
    "        # P = padding = 1\n",
    "        # s = stride = 1\n",
    "        \n",
    "        # in_channels = num_channels\n",
    "        # out_channels = number of filters\n",
    "        # kernel_size = size of convolutional kernel/filter = 3x3\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # Shape = 128, 32, 150, 150\n",
    "\n",
    "        num_channels = 32\n",
    "        # self.bn1 = nn.BatchNorm2d(num_features=num_channels)\n",
    "        # # num_features = num_channels\n",
    "        # # Shape = 128, 32, 150, 150\n",
    "\n",
    "        # # bring non-linearity\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # # Shape = 128, 32, 150, 150\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        # Reduce image size by factor of 2\n",
    "        # Shape = 128, 32, 75, 75\n",
    "\n",
    "\n",
    "        # Add second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # Shape = 128, 32, 75, 75\n",
    "\n",
    "        # self.bn2 = nn.BatchNorm2d(num_features=32)\n",
    "        # # num_features = num_channels\n",
    "        # # Shape = 128, 32, 75, 75\n",
    "\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Shape = 128, 32, 75, 75\n",
    "\n",
    "\n",
    "        # Add fully connected layer\n",
    "        self.fc1 = nn.Linear(in_features=32*75*75, out_features=num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        # First layer\n",
    "        output = self.conv1(input)\n",
    "        # output = self.bn1(output)\n",
    "        # output = self.relu1(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        # Second layer\n",
    "        output = self.conv2(output)\n",
    "        # output = self.bn2(output)\n",
    "        output = self.relu2(output)\n",
    "\n",
    "        # Feed into Fully Connected layer\n",
    "        # Above output will be matrix with shape (128, 32, 75, 75)\n",
    "        output = output.view(-1, 32*75*75)\n",
    "\n",
    "        output = self.fc1(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=6):\n",
    "        super(ConvNet,self).__init__()\n",
    "        \n",
    "        #Output size after convolution filter\n",
    "        #((w-f+2P)/s) +1\n",
    "        \n",
    "        #Input shape= (256,3,150,150)\n",
    "        \n",
    "        self.conv1=nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        # #Shape= (256,12,150,150)\n",
    "        # self.bn1=nn.BatchNorm2d(num_features=32)\n",
    "        # #Shape= (256,12,150,150)\n",
    "        self.relu1=nn.ReLU()\n",
    "        # #Shape= (256,12,150,150)\n",
    "        \n",
    "        self.pool1=nn.MaxPool2d(kernel_size=2)\n",
    "        # #Reduce the image size be factor 2\n",
    "        # #Shape= (256,12,75,75)\n",
    "        \n",
    "        self.conv2=nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        # #Shape= (256,20,75,75)\n",
    "        self.relu2=nn.ReLU()\n",
    "        # #Shape= (256,20,75,75)\n",
    "        # \n",
    "        self.pool2=nn.MaxPool2d(kernel_size=2)  \n",
    "        \n",
    "        self.fc=nn.LazyLinear(out_features=num_classes)\n",
    "        \n",
    "        #Feed forwad function\n",
    "        \n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        # output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "            \n",
    "        output=self.pool1(output)\n",
    "            \n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "        output=self.pool2(output)\n",
    "            \n",
    "        #Above output will be in matrix form, with shape (256,32,75,75)\n",
    "            \n",
    "        output=torch.flatten(output, 1)\n",
    "            \n",
    "            \n",
    "        output=self.fc(output)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_classes = 6\n",
    "\n",
    "model = ConvNet(num_classes=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.4645\n",
      "Epoch [2/20], Loss: 1.6790\n",
      "Epoch [3/20], Loss: 1.5732\n",
      "Epoch [4/20], Loss: 1.1614\n",
      "Epoch [5/20], Loss: 0.7813\n",
      "Epoch [6/20], Loss: 0.5591\n",
      "Epoch [7/20], Loss: 0.7871\n",
      "Epoch [8/20], Loss: 0.7232\n",
      "Epoch [9/20], Loss: 0.4163\n",
      "Epoch [10/20], Loss: 0.3709\n",
      "Epoch [11/20], Loss: 0.7150\n",
      "Epoch [12/20], Loss: 0.3277\n",
      "Epoch [13/20], Loss: 0.4115\n",
      "Epoch [14/20], Loss: 0.3474\n",
      "Epoch [15/20], Loss: 0.1332\n",
      "Epoch [16/20], Loss: 0.1490\n",
      "Epoch [17/20], Loss: 0.1781\n",
      "Epoch [18/20], Loss: 0.1251\n",
      "Epoch [19/20], Loss: 0.1210\n",
      "Epoch [20/20], Loss: 0.0410\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del images, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 83.60655737704919 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print(f'Accuracy of the network on the test images: {100 * correct / total} %')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
