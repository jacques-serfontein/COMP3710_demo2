{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolution Generative Adversarial Network (DCGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution was inspired by the solution developed in:\n",
    "\n",
    "https://medium.com/@manoharmanok/implementing-dcgan-in-pytorch-using-the-celeba-dataset-a-comprehensive-guide-660e6e8e29d2\n",
    "\n",
    "https://arxiv.org/pdf/1511.06434v2.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CelebA\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.utils import make_grid\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"WARNING: CUDA not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up variables\n",
    "image_size = 64         # Desired size of input images, to match the GAN generated one\n",
    "batch_size = 128        # Batch size during training\n",
    "num_samples = 1000     # Number of samples to train on\n",
    "batch_num = 1271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise data\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train data\n",
    "train_data = CelebA(root='C:/Users/Jacqu/Downloads/data/celeba', \n",
    "                    split='train', download=False, transform=transform_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random subset of data so that we can ensure our model works before moving onto the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create random indices for train and test datasets\n",
    "# train_indices = torch.randperm(len(train_data))[:num_samples]\n",
    "\n",
    "# # Create random subset of data\n",
    "# train_data = Subset(train_data, train_indices)\n",
    "\n",
    "# print(f\"Num Train Subset Samples: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for train and test datasets\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure this worked, let's show a random set of images from the dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random batch of images\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "# Make grid from the batch and normalise the images\n",
    "# Need to permute since PyTorch tensors assume image channel is first dimensions\n",
    "# but matplotlib assumes it is the third dimension\n",
    "grid = vutils.make_grid(sample_batch[0][:36], normalize=True, nrow=6).permute(1,2,0)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "\n",
    "# Convert to numpy array and plot\n",
    "plt.imshow(np.array(grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How a GAN works:\n",
    "\n",
    "![Alt text](image-1.png)\n",
    "\n",
    "So, the architecture of a GAN consists of two main components: the generator and the discriminator.\n",
    "\n",
    "### The Generator\n",
    "The generator takes random noise as input and gradually transforms it into fake images that resembles the training data. It does this through a series of layers, including:\n",
    "- Transposed convolutions\n",
    "- Batch normalisation\n",
    "- Activation functions\n",
    "These layers allow the generator to learn complex patterns and structures, resulting in it - over a number of epochs - being able to generate samples that capture the details of real data.\n",
    "\n",
    "### The Discriminator\n",
    "The discriminator acts as a binary classifier, all it needs to do is distinguish between real and generated samples. Similar to the generator, it receives input samples and passes them through:\n",
    "- Convolutional layers\n",
    "- Batch normalisation\n",
    "- Activation functions\n",
    "It's role is essentially just assessing the authenticity of samples and provide feedback (in terms of loss) to the generator.\n",
    "\n",
    "Through an adversarial training process, these two models continuously compete and improve their performance, leading to the generation of better and more realistic samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0001  # Learning rate for model during optimisation (adjusted from recommended 0.0002)\n",
    "image_channels = 3      # Number of channels in input images (3 for RGB)\n",
    "z_dim = 100             # Size of z latent space (i.e. size of generator input)\n",
    "num_epochs = 30         # Number of training epochs\n",
    "features_disc = 64      # Number of features in discriminator's convolutional layers\n",
    "features_gen = 64       # Number of features in generator's convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Generator architecture I followed looked like this:\n",
    "\n",
    "![Alt text](image-10.png)\n",
    "\n",
    "So, the input noise data, represented as a 100x1x1 tensor, undergoes a series of transposed convolutional operations to transform it into an output image of size 3x64x64 (RGB colour, with a standard size of 64).\n",
    "\n",
    "By passing the noise data through these transposed convolutional layers, the generator gradually upscales the low-dim noise into high-dim image to match desired output.\n",
    "\n",
    "The reshaping process involves increasing spatial dimensions while reducing the number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, num_features_gen):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            self._block(z_dim, num_features_gen * 16, 4, 1, 0), # output: [num_features_gen * 16, 4, 4]\n",
    "            self._block(num_features_gen * 16, num_features_gen * 8, 4, 2, 1), # output: [num_features_gen * 8, 8, 8]\n",
    "            self._block(num_features_gen * 8, num_features_gen * 4, 4, 2, 1), # output: [num_features_gen * 4, 16, 16]\n",
    "            self._block(num_features_gen * 4, num_features_gen * 2, 4, 2, 1), # output: [num_features_gen * 2, 32, 32]\n",
    "            # self._block(num_features_gen * 2, num_features_gen * 2, 4, 1, 1), # output: [num_features_gen * 2, 32, 32]\n",
    "            nn.ConvTranspose2d(\n",
    "                num_features_gen * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the Generator model is:\n",
    "\n",
    "1. Input noise: 100x1x1\n",
    "2. First transposed convolutional layer:\n",
    "    - Output size: 1024x4x4\n",
    "    - Kernel size: 4x4, stride: 1, padding: 0\n",
    "3. Second transposed convolutional layer:\n",
    "    - Output size: 512x8x8\n",
    "    - Kernel size: 4x4, stride: 2, padding: 1\n",
    "4. Third transposed convolutional layer:\n",
    "    - Output size: 256x16x16\n",
    "    - Kernel size: 4x4, stride: 2, padding: 1\n",
    "5. Fourth transposed convolutional layer:\n",
    "    - Output size: 128x32x32\n",
    "    - Kernel size: 4x4, stride: 2, padding: 1\n",
    "6. Final transposed convolutional layer:\n",
    "    - Output size: 3x64x64\n",
    "    - Kernel size: 4x4, stride: 2, padding: 1\n",
    "\n",
    "\n",
    "The Generator class represents the generator network in the DCGAN architecture. It takes as input:\n",
    "- z_dim: dimension of the latent space\n",
    "- channels_img: number of channels in the output image\n",
    "- num_features_gen: number of features for the generator\n",
    "\n",
    "This network is a sequential model, which is just a linearly stacked set of layers that directly input and output to the layer before/after it.\n",
    "\n",
    "Each sequential layer does the following:\n",
    "- Transposed convolutional layer\n",
    "- Batch normalisation\n",
    "- ReLU activation function\n",
    "\n",
    "Note that the ReLU activation function is used to add non-linearity to the model to push the model to essentially be able to create more complex images.\n",
    "Note also that transposed convolutional layers allow for upsampling, increasing the spatial dimensions of the output (compared to regular, which downsamples).\n",
    "\n",
    "The last sequential layer involves just a transposed convolutional layer.\n",
    "\n",
    "After these sequential blocks are computed, then the output image is passed through a specific type of activation function called a \"hyperbolic tangent activation function\", so that the pixel values are within the range [-1, 1].\n",
    "\n",
    "In the forward function, the input noise is just passed through the sequential layers to eventually become a generated image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- nn.Tanh is used as it:\n",
    "    - Helps the generated image match the distribution of the real image.\n",
    "    - Assists in the stability during training (especially since GANs can go a bit wild with convergence)\n",
    "- sigmoid will normalise images between [0, 1]\n",
    "- tanh will normalise images between [-1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Discriminator architecture I followed looked like this:\n",
    "\n",
    "![Alt text](image-11.png)\n",
    "\n",
    "The Discriminator takes an input image (3x64x64, same as the generated and real images) and processes\n",
    "it through a series of convolutional layers, resulting in a 1x1x1 output.\n",
    "\n",
    "The model gradually reduces the spatial dimensions while increasing the number of channels. This allows\n",
    "it to evaluate the image to determine whether it's real or not.\n",
    "\n",
    "The 1x1x1 output is a single value that represents the probability of the input image being real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, num_features_disc):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Conv2d(channels_img, num_features_disc, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._block(num_features_disc, num_features_disc * 2, 4, 2, 1),\n",
    "            self._block(num_features_disc * 2, num_features_disc * 4, 4, 2, 1),\n",
    "            self._block(num_features_disc * 4, num_features_disc * 8, 4, 2, 1),\n",
    "            nn.Conv2d(num_features_disc * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)   # negative slope of 0.2, just a slightly increased slope compared to default\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reshaping process for the Discriminator network is:\n",
    "1. Input image: 3x64x64\n",
    "2. First convolutional layer:\n",
    "    - Output size: 64x32x32\n",
    "    - Kernel size: 4x4, stride: 2, padding: 1\n",
    "3. Second convolutional layer:\n",
    "    - Output size: 128x16x16\n",
    "    - Kernel size: 4x4, stride: 2, padding: 1\n",
    "4. Third convolutional layer:\n",
    "    - Output size: 256x8x8\n",
    "    - Kernel size: 4x4, stride: 2, padding: 1\n",
    "5. Fourth convolutional layer:\n",
    "    - Output size: 512x4x4\n",
    "    - Kernel size: 4x4, stride: 2, padding: 1\n",
    "6. Final convolutional layer:\n",
    "    - Output size: 1x1x1\n",
    "    - Kernel size: 4x4, stride: 2, padding: 0\n",
    "\n",
    "The Discriminator class represents the discriminator network in the DCGAN architecture. It takes as input:\n",
    "- channels_img: number of channels in the input image\n",
    "- num_features_disc: number of features for the discriminator\n",
    "\n",
    "This network is also a sequential model, which is just a linearly stacked set of layers that directly input and output to the layer before/after it.\n",
    "\n",
    "The first layer is a conv. layer, followed by a leaky ReLU activation fucntion, to allow a small gradient for negative values (this can help the gradient flow through the network).\n",
    "\n",
    "The intermediate layers do the following:\n",
    "- Convolutional layer\n",
    "- Batch normalisation\n",
    "- Leaky ReLU activation function\n",
    "\n",
    "After these sequential layers have been computed, the final conv. layer uses a sigmoid activation function. This squashes the output between the range [0, 1], essentially classifying the input\n",
    "image as real (close to 1) or fake (close to 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialisation method used was the one recommended by the original DCGAN paper. We iterate through\n",
    "the layers of the model and initialise the weights based on the layer type.\n",
    "- Convolutional: Initialise the weights with a mean of 0 and standard devation of 0.02\n",
    "- Batch Norm: Initialise weights with a mean of 1 and a standard deviation of 0.02, set bias to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following initialisation method was developed using the original DCGAN paper and ChatGPT.\n",
    "def initialise_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(model.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(model.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(model.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we initialise our models with our parameters defined earlier and send them to the GPU.\n",
    "We then initialise the weights of the models with our previous function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(z_dim, image_channels, features_gen).to(device)\n",
    "discriminator = Discriminator(image_channels, features_disc).to(device)\n",
    "\n",
    "initialise_weights(generator)\n",
    "initialise_weights(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the DCGAN paper again, the Adam optimiser was used for both models with the\n",
    "recommended learning rate of 0.0002 (as defined previously). Additionally, the beta_1 momentum term\n",
    "was reduced to 0.5 as suggested by the paper. The default value of 0.9 was said to have resulted in\n",
    "training oscillation.\n",
    "\n",
    "Next, the Binary Cross Entropy Loss function was used to compare the predictions of the Discriminator\n",
    "with the true labels. This was used due to the commonality of the function within GANs.\n",
    "\n",
    "Lastly, some random noise was created to act as the input noise for the generator during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(32, z_dim, 1, 1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a utility function that takes a tensor of images, normalises them, creates a grid\n",
    "of images and displays them using matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function was taken directly from the DCGAN tutorial notebook as it was seen\n",
    "# to be a utility function.\n",
    "def show_tensor_images(image_tensor, num_images=32, size=(1, 64, 64)):\n",
    "    image_tensor = (image_tensor + 1) / 2  # Unnormalize the image\n",
    "    image_unflat = image_tensor.detach().cpu()  # Detach and move to CPU\n",
    "\n",
    "    # Calculate the number of rows needed for the number of images\n",
    "    nrow = math.ceil(math.sqrt(num_images))\n",
    "\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)  # Make the grid\n",
    "    \n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())  # Rearrange dimensions and plot\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for batch_idx, (real_im, _ ) in enumerate(train_loader):\n",
    "        real_im = real_im.to(device)\n",
    "\n",
    "        # 1. Noise generation and create fake image\n",
    "        noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)\n",
    "        fake_im = generator(noise)\n",
    "\n",
    "        # 2. Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        # 2.1 Calculate loss on all-real batch\n",
    "        disc_real = discriminator(real_im).reshape(-1) # reshape to 1D vector\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "\n",
    "        # 2.2 Calculate loss on all-fake batch\n",
    "        fake_im = generator(noise)\n",
    "        disc_fake = discriminator(fake_im.detach()).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "\n",
    "        # 2.3 Calculate average discriminator loss\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "\n",
    "        # 2.4 Backprop and optimise discriminator\n",
    "        discriminator.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        # 3. Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        # 3.1 Calculate loss on all-fake batch\n",
    "        output = discriminator(fake_im).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "\n",
    "        # 3.2 Backprop and optimise generator\n",
    "        generator.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Print losses occasionally and fake images occasionally\n",
    "        if (epoch % 1 == 0) and (batch_idx == batch_num):\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(train_loader)} \\\n",
    "                    Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "            # with torch.no_grad():\n",
    "            #     fake_im = generator(fixed_noise)\n",
    "            #     img_grid_fake = make_grid(fake_im[:32], normalize=True)\n",
    "            #     show_tensor_images(img_grid_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save our model for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator, 'gen.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.load('gen.pth')\n",
    "with torch.no_grad():\n",
    "    fake_im = generator(fixed_noise)\n",
    "    img_grid_fake = make_grid(fake_im[:32], normalize=True)\n",
    "    show_tensor_images(img_grid_fake)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
