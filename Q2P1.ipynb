{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://www.youtube.com/watch?v=9OHlgDjaE2I&ab_channel=AI-SPECIALS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy paste from Q1 to get the LFW data subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.images\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# normalise according to spec\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_train = X_train[:, np.newaxis, :, :]\n",
    "X_test = X_test[:, np.newaxis, :, :]\n",
    "\n",
    "# Convert to torch floats\n",
    "x_train_to_tensor = torch.from_numpy(X_train).to(torch.float32)\n",
    "y_train_to_tensor = torch.from_numpy(y_train).to(torch.long) \n",
    "x_test_to_tensor = torch.from_numpy(X_test).to(torch.float32)\n",
    "y_test_to_tensor = torch.from_numpy(y_test).to(torch.long)\n",
    "\n",
    "# Convert to tensors\n",
    "train_dataset = TensorDataset(x_train_to_tensor, y_train_to_tensor)\n",
    "test_dataset = TensorDataset(x_test_to_tensor, y_test_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 1, 50, 37])\n"
     ]
    }
   ],
   "source": [
    "inputs, label = next(iter(train_loader))\n",
    "print(f'Input shape: {inputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for each index in shape:\n",
    "- 0 tells us batch size\n",
    "- 1 tells us number of input channels\n",
    "- 2 tells us height of image\n",
    "- 3 tells us width of image\n",
    "So we have a batch size of 64, 1 input channel (grayscale image), 50x37 image shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define our constants from the spec sheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "num_filters = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can determine each shape of the CNN by using the formula:\n",
    "\n",
    "$$ Output \\space Shape = {{{W-K+2P} \\over S} + 1} $$\n",
    "\n",
    "Where:\n",
    "- W = width\n",
    "- K = kernel size\n",
    "- P = padding (default = 0)\n",
    "- S = stride (default = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can work out the shape of our CNN before doing anything:\n",
    "\n",
    "Layer 1:\n",
    "- Input shape = (64, 1, 50, 37)\n",
    "- Output shape = (64, 32, 48, 35)\n",
    "    - Height = (50 - 3 + 0) / 1 + 1 = 48\n",
    "    - Width = (37 - 3 + 1) / 1 + 1 = 35\n",
    "\n",
    "Layer 2:\n",
    "- Input shape = (64, 32, 48, 35)\n",
    "- Output shape = (64, 32, 46, 33)\n",
    "    - Height = (48 - 3 + 0) / 1 + 1 = 46\n",
    "    - Width = (35 - 3 + 1) / 1 + 1 = 33\n",
    "\n",
    "Fully connected layer:\n",
    "- Shape gets flattened first:\n",
    "    - Output Shape = (64, 32 * 46 * 33) = (64, 48432)\n",
    "- LazyLinear infers the output shape based on whatever we input, in this case I used the number of classes, so:\n",
    "    - Output Shape = (64, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=kernel_size)\n",
    "\n",
    "        # Add non-linear activation function\n",
    "        self.relu1=nn.ReLU()\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=num_filters, out_channels=num_filters, kernel_size=kernel_size)\n",
    "\n",
    "        # Add non-linear activation function\n",
    "        self.relu2=nn.ReLU()\n",
    "\n",
    "        # Add fully connected layer as per the spec\n",
    "        self.fc=nn.LazyLinear(out_features=num_classes)\n",
    "\n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.relu1(output)\n",
    "\n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "\n",
    "        output=torch.flatten(output, 1)\n",
    "        output=self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jacqu\\miniconda3\\envs\\conda-torch\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# Define hyper-parameters arbitrarily\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 0.001 # this is a generally accepted value for learning rate\n",
    "num_classes = 6\n",
    "\n",
    "# Initialise the model and send to GPU\n",
    "model = CNN(num_classes=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001) # Used default weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2.0769\n",
      "Epoch [2/20], Loss: 2.0867\n",
      "Epoch [3/20], Loss: 1.7113\n",
      "Epoch [4/20], Loss: 1.4675\n",
      "Epoch [5/20], Loss: 1.2444\n",
      "Epoch [6/20], Loss: 0.9398\n",
      "Epoch [7/20], Loss: 1.0364\n",
      "Epoch [8/20], Loss: 0.5101\n",
      "Epoch [9/20], Loss: 0.3737\n",
      "Epoch [10/20], Loss: 0.5475\n",
      "Epoch [11/20], Loss: 0.5690\n",
      "Epoch [12/20], Loss: 0.4076\n",
      "Epoch [13/20], Loss: 0.3557\n",
      "Epoch [14/20], Loss: 0.3830\n",
      "Epoch [15/20], Loss: 0.2110\n",
      "Epoch [16/20], Loss: 0.2360\n",
      "Epoch [17/20], Loss: 0.1621\n",
      "Epoch [18/20], Loss: 0.5368\n",
      "Epoch [19/20], Loss: 0.0643\n",
      "Epoch [20/20], Loss: 0.3501\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del images, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 82.29508196721312 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print(f'Accuracy of the network on the test images: {100 * correct / total} %')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
